{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {},
      "source": [
        "# 导包\r\n",
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.types import StringType,StructField,StructType\r\n",
        "from datetime import datetime,timedelta    \r\n",
        "import pandas as pd\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 方法\r\n",
        "def schema_from_pandas(df):\r\n",
        "    \"\"\"读取pd文件schema\"\"\"\r\n",
        "    schema = StructType([StructField(str(col),StringType(),True) for col in df.columns]) \r\n",
        "    return schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# todo 1:数据载入\r\n",
        "read_path = 'abfss://data-warehouse-ods@dlsaaddpnorth3001.dfs.core.chinacloudapi.cn/ods_mau_travel_app_cn_mf.csv'\r\n",
        "df_pd = pd.read_csv(read_path)\r\n",
        "\r\n",
        "schema = schema_from_pandas(df_pd)\r\n",
        "df = spark.createDataFrame(df_pd, schema=schema)\r\n",
        "\r\n",
        "# todo 2: 数据处理\r\n",
        "formatted_endtime = datetime.now()\r\n",
        "formatted_endtime += timedelta(hours=8)\r\n",
        "etl_load_time = formatted_endtime.strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
        "\r\n",
        "\r\n",
        "df = df.withColumnRenamed('Key','TA_key') \\\r\n",
        "       .select('TA_key','trip_itinerary') \\\r\n",
        "       .withColumn(\"trip_itinerary\", F.explode(F.split(F.split(df['trip_itinerary'], 'Tools\\* ').getItem(1), \"\\n\"))) \\\r\n",
        "       .where(F.col('trip_itinerary') != '') \\\r\n",
        "       .withColumn('from_place', F.split(F.col('trip_itinerary'),'\\|').getItem(0)) \\\r\n",
        "       .withColumn('from_date', F.split(F.col('trip_itinerary'),'\\|').getItem(1)) \\\r\n",
        "       .withColumn('to_place', F.split(F.col('trip_itinerary'),'\\|').getItem(3)) \\\r\n",
        "       .withColumn('to_date', F.split(F.col('trip_itinerary'),'\\|').getItem(4)) \\\r\n",
        "       .withColumn('transportation_tools', F.split(F.col('trip_itinerary'),'\\|').getItem(6)) \\\r\n",
        "       .withColumn('etl_load_time', F.lit(etl_load_time)) \\\r\n",
        "       .drop('trip_itinerary') \\\r\n",
        "       .toPandas()\r\n",
        "\r\n",
        "display(df)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# todo 3:数据写入\r\n",
        "save_path = 'abfss://data-warehouse-dwd@dlsaaddpnorth3001.dfs.core.chinacloudapi.cn/dwd_fi_te_travel_app_item_cn.csv'\r\n",
        "df.to_csv(save_path,index=False,header=True)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}