{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-22T05:17:48.4132293Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-22T05:17:50.4784473Z",
              "execution_finish_time": "2024-01-22T05:17:50.4785976Z",
              "spark_jobs": null,
              "parent_msg_id": "266ceda6-06cd-4676-8021-259cb27565a7"
            },
            "text/plain": "StatementMeta(, , -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "请设置receiver_email、subject、body，以发送邮件\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%run TEST/WZX/Test/send_email"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "aasparkpool001",
              "session_id": "898",
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-22T05:14:32.3478502Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-22T05:14:32.5323449Z",
              "execution_finish_time": "2024-01-22T05:14:36.6640354Z",
              "spark_jobs": null,
              "parent_msg_id": "6e83be43-c0db-4a09-afcd-26f0fb40f9b3"
            },
            "text/plain": "StatementMeta(aasparkpool001, 898, 4, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 导包\r\n",
        "from notebookutils import mssparkutils  \r\n",
        "from openpyxl import load_workbook\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "from datetime import datetime,timedelta    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "aasparkpool001",
              "session_id": "898",
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-22T05:14:38.3566479Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-22T05:14:38.5111227Z",
              "execution_finish_time": "2024-01-22T05:14:51.4368127Z",
              "spark_jobs": null,
              "parent_msg_id": "f3c7a141-0b0a-4aec-b489-5b8df39d15e5"
            },
            "text/plain": "StatementMeta(aasparkpool001, 898, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "execution_count": 11,
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 挂载blob\r\n",
        "mssparkutils.fs.mount(\r\n",
        "    \"wasbs://bosch-dw-integration-layer@proddataplatcn3blob01.blob.core.chinacloudapi.cn\",\r\n",
        "    \"/mnt/bosch-dw-integration-layer\",\r\n",
        "    {\"LinkedService\" : \"AzureBlobStorage_proddataplatcn3blob01\"}\r\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "aasparkpool001",
              "session_id": "898",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-22T05:14:54.0347818Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-22T05:14:54.1822591Z",
              "execution_finish_time": "2024-01-22T05:14:54.3591867Z",
              "spark_jobs": null,
              "parent_msg_id": "604edfa4-d0ee-44d7-ac36-fbc7feecadd7"
            },
            "text/plain": "StatementMeta(aasparkpool001, 898, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "receiver_email = ['external.Zhaoxiang.Wang@cn.bosch.com','fei.zhao2@cn.bosch.com', 'zhaoxiang_wang@leansight.cn'] # set email \r\n",
        "subject = 'Bosch Job Alarm: Travel Expense'\r\n",
        "body = 'Travel Expense 报销数据未更新, 请及时确认————This is an alarm email from python. Please do not reply'\r\n",
        "body1 = 'Travel Expense 报销数据增加列, 请及时确认————This is an alarm email from python. Please do not reply'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "aasparkpool001",
              "session_id": "898",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-22T05:36:55.9814788Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-22T05:36:56.1239474Z",
              "execution_finish_time": "2024-01-22T05:36:56.3064377Z",
              "spark_jobs": null,
              "parent_msg_id": "d82615f0-af4b-464b-aaa3-f69e22749197"
            },
            "text/plain": "StatementMeta(aasparkpool001, 898, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "==================travel expense: Reimbursement=====================\n['Reimbursement', '20230101', '20231130.xlsx']\n['Reimbursement', '20231201', '20231231.xlsx']\n['WorkON+Platform(16).xlsx']\n/synfs/898/mnt/bosch-dw-integration-layer/manual/AA_CTG/Reimbursement/Reimbursement_20231201_20231231.xlsx\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print('==================travel expense: Reimbursement=====================')\r\n",
        "def get_last_month():\r\n",
        "    \"\"\" \r\n",
        "     * @ message : 获取上月与本月月份名\r\n",
        "     * @ return   [list] 月份\r\n",
        "    \"\"\"\r\n",
        "    current_date = datetime.now()\r\n",
        "    current_year = current_date.year\r\n",
        "    current_month = current_date.month\r\n",
        "    previous_month = current_month - 1 if current_month > 1 else 12\r\n",
        "    previous_year = current_year if current_month > 1 else current_year -1\r\n",
        "    last_month = str(previous_year) + str(previous_month).zfill(2)\r\n",
        "    now_month = str(current_year) + str(current_month).zfill(2)\r\n",
        "    month_list = [last_month] # remove current month\r\n",
        "    return month_list\r\n",
        "\r\n",
        "\r\n",
        "def data_need(blobs):\r\n",
        "    \"\"\" \r\n",
        "     * @ message : 获取需要的blob\r\n",
        "     * @ param2   [type] blobs: blobs\r\n",
        "     * @ return   [type]\r\n",
        "    \"\"\"\r\n",
        "    need_blobs = []\r\n",
        "    for i in blobs:\r\n",
        "        print(i.split('_'))\r\n",
        "        split_blob = i.split('_')\r\n",
        "        if len(split_blob) > 2:\r\n",
        "            blob_name = i.split('_')[-2][:6]\r\n",
        "        else:\r\n",
        "            blob_name = i\r\n",
        "        if blob_name not in get_last_month():\r\n",
        "            continue\r\n",
        "        need_blobs.append(i)\r\n",
        "    return need_blobs\r\n",
        "\r\n",
        "def hand_file():\r\n",
        "    path = mssparkutils.fs.getMountPath(\"/mnt/bosch-dw-integration-layer\")\r\n",
        "    root = path + '/manual/AA_CTG/Reimbursement'\r\n",
        "    files = os.listdir(root)\r\n",
        "    files = data_need(files) # enble filter\r\n",
        "    try:\r\n",
        "        file_path = os.path.join(root,files[0])\r\n",
        "        print(file_path)\r\n",
        "        return file_path\r\n",
        "    except:\r\n",
        "        print('=======未识别到上月报销数据=========')\r\n",
        "        send_to_email(receiver_email, subject, body)\r\n",
        "        mssparkutils.notebook.exit()\r\n",
        "file_path = hand_file()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "aasparkpool001",
              "session_id": "898",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-22T05:37:08.8063466Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-22T05:37:08.9466533Z",
              "execution_finish_time": "2024-01-22T05:39:43.761858Z",
              "spark_jobs": null,
              "parent_msg_id": "fe345a7b-59f5-431b-936b-e2e02073bec2"
            },
            "text/plain": "StatementMeta(aasparkpool001, 898, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_8290/1030961550.py:104: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df.iloc[i, j] = df.iloc[i - 1,j]\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def unmerge_and_copy(sheet): # 拆分单元格\r\n",
        "    merged_cells = sheet.merged_cells.ranges    # 复制一份合并单元格集合的副本\r\n",
        "    for merged_cell in merged_cells.copy():        # 遍历每个合并单元格\r\n",
        "        merged_value = sheet.cell(row=merged_cell.min_row, column=merged_cell.min_col).value     # 获取合并单元格的值\r\n",
        "        sheet.unmerge_cells(str(merged_cell))    # 拆分合并单元格\r\n",
        "        for row in range(merged_cell.min_row, merged_cell.max_row + 1): # 将值分配给每个子单元格\r\n",
        "            for column in range(merged_cell.min_col, merged_cell.max_col + 1):\r\n",
        "                cell = sheet.cell(row=row, column=column)\r\n",
        "                cell.value = merged_value\r\n",
        " \r\n",
        "# 共性列名\r\n",
        "columns_cols = [\r\n",
        "    'Project'\r\n",
        "    ,'Key'\r\n",
        "    ,'Summary'\r\n",
        "    ,'Issue Type'\r\n",
        "    ,'Status'\r\n",
        "    ,'Priority'\r\n",
        "    ,'Resolution'\r\n",
        "    ,'Current Assignee'\r\n",
        "    ,'Applicant'\r\n",
        "    ,'Created'\r\n",
        "    ,'Updated'\r\n",
        "    ,'Due Date'\r\n",
        "    ,'NT-ID Second Level'\r\n",
        "    ,'NT-ID First Level'\r\n",
        "    ,'Application reason & additional comment'\r\n",
        "    ,'Cost Center'\r\n",
        "    ,'Company Code'\r\n",
        "    ,'Special Approval *'\r\n",
        "    ,'Adjustment necessary *'\r\n",
        "    ,'Approvers *'\r\n",
        "    ,'Advance applied or not *'\r\n",
        "    ,'Approved Travel Application'\r\n",
        "    ,'Approved Advance Application'\r\n",
        "    ,'Charge to other cost center *'\r\n",
        "    ,'apcntr.field.trainingProvider'\r\n",
        "    ,'apcntr.field.trainingType'\r\n",
        "    ,'apcntr.field.trainingNeedsCaused'\r\n",
        "    ,'Name (Modify & Post)'\r\n",
        "    ,'apcntr.field.thisTrainingCost'\r\n",
        "    ,'apcntr.field.tmpTrainingType'\r\n",
        "    ,'Open advance'\r\n",
        "    ,'Time (Modify & Post)'\r\n",
        "    ,'apcntr.field.estimatedTravelExpenses'\r\n",
        "    ,'apcntr.field.participantNumber'\r\n",
        "    ,'apcntr.field.estimatedOtherCost'\r\n",
        "    ,'apcntr.field.estimatedTotalCost'\r\n",
        "    ,'common.field.employee.companycode'\r\n",
        "    ,'Total - FIN Deduction'\r\n",
        "    ,'Total - Kilometers'\r\n",
        "    ,'Global CC2'\r\n",
        "    ,'Policy Compliance Result'\r\n",
        "    ,'Non-compliance Reason'\r\n",
        "    ,'Personnel number *'\r\n",
        "    ,'Tel.'\r\n",
        "    ,'FIN Remark Ref'\r\n",
        "    ,'FIN Remark Reason Type'\r\n",
        "    ,'FIN Remark Reason Details'\r\n",
        "    ,'FIN Remark Reason Code'\r\n",
        "    ,'FIN Remark Comment'\r\n",
        "    ,'Actual Start Date, Time*:'\r\n",
        "    ,'SAP number'\r\n",
        "    ,'Charge to Cost Center *'\r\n",
        "    ,'Travel or not *'\r\n",
        "    ,'Charge to other division (company code)'\r\n",
        "    ,'Travel Type'\r\n",
        "    ,'Planned End Date:'\r\n",
        "    ,'Planned Start Date:'\r\n",
        "    ,'Planned Duration:'\r\n",
        "    ,'WBS/Internal order Posted'\r\n",
        "    ,'Total - Payable'\r\n",
        "    ,'Org.Unit'\r\n",
        "    ,'Message returned from SAP'\r\n",
        "    ,'Actual Duration:'\r\n",
        "    ,'Name'\r\n",
        "    ,'Actual End Date, Time*:'\r\n",
        "    ,'WBS/Internal order'\r\n",
        "    ,'Global CC'\r\n",
        "    ,'Applicant NT/Display Name'\r\n",
        "    ,'Travel to*:'\r\n",
        "    ,'travel_allow'\r\n",
        "    ,'WBS or Internal order'\r\n",
        "    ,'Total - Claim'\r\n",
        "    ,'Request need print and delivery?'\r\n",
        "    ,'Fapiao & Supporting'\r\n",
        "    ,'Total - Actual payment'\r\n",
        "    ,'Text sent to SAP *'\r\n",
        "]\r\n",
        "# 加载Excel文件\r\n",
        "workbook = load_workbook(file_path)\r\n",
        "sheet = workbook.active\r\n",
        "unmerge_and_copy(sheet)\r\n",
        "\r\n",
        "# 处理数据\r\n",
        "df_pd = pd.DataFrame(sheet.values,dtype=str)\r\n",
        "df_pd.columns = df_pd.iloc[3]\r\n",
        "df = df_pd.iloc[4:-1,:]\r\n",
        "\r\n",
        "merge_index = [df.columns.get_loc(col) for col in columns_cols]\r\n",
        "for i in range(len(df)):    \r\n",
        "    for j in merge_index:  \r\n",
        "        if df.iloc[i, j] == '' or df.iloc[i,j] is None: # 如果某个值为空值，则复制上一行此列的数据\r\n",
        "            df.iloc[i, j] = df.iloc[i - 1,j]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "aasparkpool001",
              "session_id": "898",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2024-01-22T05:43:09.0351614Z",
              "session_start_time": null,
              "execution_start_time": "2024-01-22T05:43:09.1910086Z",
              "execution_finish_time": "2024-01-22T05:43:18.0816002Z",
              "spark_jobs": null,
              "parent_msg_id": "99c89018-e934-49eb-a2b2-a8b811cf8973"
            },
            "text/plain": "StatementMeta(aasparkpool001, 898, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_8290/167285046.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df1['etl_load_time'] = etl_load_time\n"
        }
      ],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "ods_path = 'abfss://data-warehouse-ods@dlsaaddpnorth3001.dfs.core.chinacloudapi.cn/ods_mau_reimburse_cn_mf.csv' # path\r\n",
        "di_path = 'abfss://data-warehouse-ods@dlsaaddpnorth3001.dfs.core.chinacloudapi.cn/ods_mau_reimburse_cn_mi.csv'\r\n",
        "formatted_endtime = datetime.now()\r\n",
        "formatted_endtime += timedelta(hours=8)\r\n",
        "etl_load_time = formatted_endtime.strftime(f\"%Y-%m-%d %H:%M:%S\")    # etl_load_time\r\n",
        "\r\n",
        "df1 = df\r\n",
        "df1['etl_load_time'] = etl_load_time\r\n",
        "df1.to_csv(di_path,index=False, header=True)    # 保持数据格式一致性\r\n",
        "\r\n",
        "df1 = pd.read_csv(di_path,dtype=str).drop('etl_load_time',axis=1) \r\n",
        "ods_df = pd.read_csv(ods_path,dtype=str).drop('etl_load_time',axis=1) # read\r\n",
        "\r\n",
        "try:\r\n",
        "    result = pd.concat([ods_df, df1]).reset_index(drop=True).drop_duplicates()  # handle\r\n",
        "except Exception as e:\r\n",
        "    print(f'===========源数据增加列{e}============')\r\n",
        "    send_to_email(receiver_email, subject, body1)\r\n",
        "    mssparkutils.notebook.exit()\r\n",
        "\r\n",
        "result['etl_load_time'] = etl_load_time\r\n",
        "\r\n",
        "mssparkutils.fs.rm(di_path)\r\n",
        "result.to_csv(ods_path,index=False, header=True) # save\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  }
}