{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "collapsed": false
      },
      "source": [
        "# todo 1: 读取数据\r\n",
        "employee_path = 'abfss://data-warehouse-ods@dlsaaddpnorth3001.dfs.core.chinacloudapi.cn/ods_mau_employee_cn_mf.csv'\r\n",
        "travel_path = 'abfss://data-warehouse-dwd@dlsaaddpnorth3001.dfs.core.chinacloudapi.cn/dwd_fi_te_travel_app_header_cn.parquet' \r\n",
        "reimburse_path = 'abfss://data-warehouse-dwd@dlsaaddpnorth3001.dfs.core.chinacloudapi.cn/dwd_fi_te_reimburse_header.parquet' \r\n",
        "air_path = 'abfss://data-warehouse-dwd@dlsaaddpnorth3001.dfs.core.chinacloudapi.cn/dwd_fi_te_air_ticket_cn.csv' \r\n",
        " \r\n",
        "\r\n",
        "df_emp = spark.read.csv(employee_path,header=True) \\\r\n",
        "                .where(F.col('Personnel number *').isNotNull()) \\\r\n",
        "                .withColumn('source',F.lit(1)) \\\r\n",
        "                .withColumnRenamed('Org.Unit','org_unit') \\\r\n",
        "                .withColumn('rn',F.row_number().over(Window.partitionBy('Personnel number *').orderBy('org_unit','name'))) \\\r\n",
        "                .where(F.col('rn')==1) \\\r\n",
        "                .dropDuplicates()\r\n",
        "\r\n",
        "df_travel = spark.read.parquet(travel_path,header=True) \\\r\n",
        "                .select(F.col('personnel_number').alias('t_no'),F.col('org_unit').alias('t_org'),F.col('name').alias('t_name')) \\\r\n",
        "                .withColumn('source',F.lit(2)) \\\r\n",
        "                .withColumn('rn',F.row_number().over(Window.partitionBy('t_no').orderBy('t_org','t_name'))) \\\r\n",
        "                .where(F.col('rn')==1) \\\r\n",
        "                .dropDuplicates() \r\n",
        "\r\n",
        "df_reimburse = spark.read.parquet(reimburse_path,header=True) \\\r\n",
        "                .select(F.col('personnel_number').alias('r_no'),F.col('org_unit').alias('r_org'),F.col('name').alias('r_name')) \\\r\n",
        "                .withColumn('source',F.lit(3)) \\\r\n",
        "                .withColumn('rn',F.row_number().over(Window.partitionBy('r_no').orderBy('r_org','r_name'))) \\\r\n",
        "                .where(F.col('rn')==1) \\\r\n",
        "                .dropDuplicates() \r\n",
        "\r\n",
        "df_air = spark.read.csv(air_path,header=True) \\\r\n",
        "                .select(F.col('employee_no').alias('air_no'),F.col('department').alias('air_org'),F.col('name').alias('air_name')) \\\r\n",
        "                .dropDuplicates() \\\r\n",
        "                .withColumn('source',F.lit(4)) \\\r\n",
        "                .withColumn('rn',F.row_number().over(Window.partitionBy('air_no').orderBy('air_org','air_name'))) \\\r\n",
        "                .where(F.col('rn')==1) \\\r\n",
        "                .dropDuplicates() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# todo 2 : 主数据\r\n",
        "df_no = df_emp.unionAll(df_travel).unionAll(df_reimburse).unionAll(df_air) \\\r\n",
        "              .select('Personnel number *',\r\n",
        "                    F.min('source').over(Window.partitionBy('Personnel number *').orderBy('source')).alias('from')\r\n",
        "                ) \\\r\n",
        "              .withColumnRenamed('Personnel number *','personnel_number') \\\r\n",
        "              .where(F.col('personnel_number').isNotNull() & ~(F.col('personnel_number') == 0)) \\\r\n",
        "              .dropDuplicates() \r\n",
        "\r\n",
        "df_master = df_no.join(df_emp,df_no['personnel_number']==df_emp['Personnel number *'],'left') \\\r\n",
        "                 .join(df_travel,df_no['personnel_number']==df_travel['t_no'],'left') \\\r\n",
        "                 .join(df_reimburse,df_no['personnel_number']==df_reimburse['r_no'],'left') \\\r\n",
        "                 .join(df_air,df_no['personnel_number']==df_air['air_no'],'left') \r\n",
        "\r\n",
        "df_result = df_master.select('personnel_number',\r\n",
        "                    F.when(F.col('org_unit').isNotNull(),F.col('org_unit'))\r\n",
        "                    .when(F.col('t_org').isNotNull(),F.col('t_org'))\r\n",
        "                    .when(F.col('r_org').isNotNull(),F.col('r_org'))\r\n",
        "                    .when(F.col('air_org').isNotNull(),F.col('air_org'))\r\n",
        "                    .otherwise('').alias('org_unit'),\r\n",
        "\r\n",
        "                    F.when(F.col('name').isNotNull(),F.col('name'))\r\n",
        "                    .when(F.col('t_name').isNotNull(),F.col('t_name'))\r\n",
        "                    .when(F.col('r_name').isNotNull(),F.col('r_name'))\r\n",
        "                    .when(F.col('air_name').isNotNull(),F.col('air_name'))\r\n",
        "                    .otherwise('').alias('employee_name'),\r\n",
        "\r\n",
        "                    F.when(F.col('from')==1,'yes').otherwise('no').alias('onboard_or_not')\r\n",
        "                    )\r\n",
        "\r\n",
        "display(df_result)\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# todo 3: 写入文件\r\n",
        "save_path = 'abfss://data-warehouse-dim@dlsaaddpnorth3001.dfs.core.chinacloudapi.cn/dim_employee_mf.csv'\r\n",
        "df_result.toPandas().to_csv(save_path, mode='w',index=False, header=True)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}